{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Aligning & Scaling Point Clouds\n",
    "\n",
    "In order to accurately scale the 3D model point cloud and have it be aligned with the scan cloud, you'll need to align their orientation first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinect_scan_path = \"C:\\\\Users\\\\gusta\\\\Desktop\\\\ITESM_Desktop\\\\maestria\\\\tesis\\\\TercerSemestre\\\\realTimeICP\\\\ImagesAndPointClouds\\\\pointcloud\\\\new_pc\\\\0_color.ply\"\n",
    "model_path = \"C:\\\\Users\\\\gusta\\\\Desktop\\\\ITESM_Desktop\\\\maestria\\\\tesis\\\\TercerSemestre\\\\realTimeICP\\\\refined_one_img_process\\\\results\\\\model_point_cloud\\\\output_10000.ply\"\n",
    "target = o3d.io.read_point_cloud(kinect_scan_path) \n",
    "source = o3d.io.read_point_cloud(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Centroid to Defining feature line\n",
    "\n",
    "The following section intends to make use of a line that goes from the centroid of the pepper model to the peduncle in order to use it to align the scan with model. For the kinect scan the peduncle wasn't as easy to reach, so we went with a defining feature, which traversed the scan length wise, giving a sense of its orientation. We use the Z coordinate for the model since the peduncle is located on the highest point of the Z axis. For the scan we use the y-axis due to the way the model is projected into 3D space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroid(point_cloud): \n",
    "    return np.mean(point_cloud.points, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_peduncle_point(pcd): \n",
    "    z_coordinates = np.asarray(pcd.points)[:,2]\n",
    "\n",
    "    threshold = np.percentile(z_coordinates, 98)\n",
    "    top_points = np.asarray(pcd.points)[z_coordinates > threshold]\n",
    "\n",
    "    return np.mean(top_points, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_peduncle_point_scan(pcd): \n",
    "    y_coordinates = np.asarray(pcd.points)[:,0]\n",
    "\n",
    "    threshold = np.percentile(y_coordinates, 98)\n",
    "    top_points = np.asarray(pcd.points)[y_coordinates > threshold]\n",
    "\n",
    "    return np.mean(top_points, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_line(pcd):\n",
    "    centroid = compute_centroid(pcd)\n",
    "    peduncle_point = identify_peduncle_point(pcd)\n",
    "\n",
    "    direction_vector = peduncle_point - centroid\n",
    "    normalized_vector = direction_vector / np.linalg.norm(direction_vector)\n",
    "    return normalized_vector, centroid, peduncle_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_line_scan(pcd):\n",
    "    centroid = compute_centroid(pcd)\n",
    "    peduncle_point = identify_peduncle_point_scan(pcd)\n",
    "\n",
    "    direction_vector = peduncle_point - centroid\n",
    "    normalized_vector = direction_vector / np.linalg.norm(direction_vector)\n",
    "    return normalized_vector, centroid, peduncle_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction, centroid, peduncle_point = compute_line(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_points = [centroid, peduncle_point]\n",
    "line_set = o3d.geometry.LineSet()\n",
    "line_set.points = o3d.utility.Vector3dVector(line_points)\n",
    "line_set.lines = o3d.utility.Vector2iVector([[0, 1]])\n",
    "o3d.visualization.draw_geometries([source, line_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As proven by the visualization, we can see the line from the center of the pepper model all the way up to the peduncle. This gives us a good idea of a rotation axis which we can use to orient it with the model scan more easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_scan, centroid_scan, peduncle_point_scan = compute_line_scan(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_points = [centroid_scan, peduncle_point_scan]  \n",
    "line_set = o3d.geometry.LineSet()\n",
    "line_set.points = o3d.utility.Vector3dVector(line_points)\n",
    "line_set.lines = o3d.utility.Vector2iVector([[0, 1]])\n",
    "o3d.visualization.draw_geometries([target, line_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scan, the line goes to the opposite sense of the peduncle. Still it gives us a good take on the orientation of the pepper scan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The origin of the scanned point cloud is not in the pepper. It is in one of the corners of the image. In order to make working with the scan easier, we moved the origin to the centroid of the pepper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Load original point cloud\n",
    "pcd = o3d.io.read_point_cloud(kinect_scan_path)\n",
    "\n",
    "# Compute centroid\n",
    "centroid = np.mean(np.asarray(pcd.points), axis=0)\n",
    "\n",
    "# Create point cloud for the centroid\n",
    "centroid_pcd = o3d.geometry.PointCloud()\n",
    "centroid_pcd.points = o3d.utility.Vector3dVector([centroid])\n",
    "\n",
    "# Assign red color to the centroid\n",
    "centroid_color = [1, 0, 0]  # RGB for red\n",
    "centroid_pcd.colors = o3d.utility.Vector3dVector([centroid_color])\n",
    "\n",
    "# Create a coordinate frame (axes) at the origin (can be adjusted to another position)\n",
    "coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])\n",
    "\n",
    "# Visualize everything together\n",
    "o3d.visualization.draw_geometries([pcd, centroid_pcd, coord_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Load original point cloud\n",
    "pcd = o3d.io.read_point_cloud(kinect_scan_path)\n",
    "\n",
    "# Compute centroid\n",
    "centroid = np.mean(np.asarray(pcd.points), axis=0)\n",
    "\n",
    "# Translate point cloud to make the centroid the origin\n",
    "translated_points = np.asarray(pcd.points) - centroid\n",
    "pcd.points = o3d.utility.Vector3dVector(translated_points)\n",
    "\n",
    "# Create point cloud for the centroid\n",
    "centroid_pcd = o3d.geometry.PointCloud()\n",
    "centroid_pcd.points = o3d.utility.Vector3dVector([centroid])\n",
    "\n",
    "# Assign red color to the centroid\n",
    "centroid_color = [1, 0, 0]  # RGB for red\n",
    "centroid_pcd.colors = o3d.utility.Vector3dVector([centroid_color])\n",
    "\n",
    "# Create a coordinate frame (axes) at the origin (can be adjusted to another position)\n",
    "coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])\n",
    "\n",
    "# Save and/or visualize the translated point cloud\n",
    "o3d.io.write_point_cloud(\"C:\\\\Users\\\\gusta\\\\Desktop\\\\ITESM_Desktop\\\\maestria\\\\tesis\\\\TercerSemestre\\\\realTimeICP\\\\refined_one_img_process\\\\results\\\\translated_kinect_scan\\\\translated_kinect_scan.ply\", pcd)\n",
    "o3d.visualization.draw_geometries([pcd, centroid_pcd, coord_frame])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having moved the origin to the centroid, we can now start the alignment operations. We compute the center line again first, just to make sure we're doing the alignment right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_kinect_scan_path = \"C:\\\\Users\\\\gusta\\\\Desktop\\\\ITESM_Desktop\\\\maestria\\\\tesis\\\\TercerSemestre\\\\realTimeICP\\\\refined_one_img_process\\\\results\\\\translated_kinect_scan\\\\translated_kinect_scan.ply\"\n",
    "translated_kinect_scan = o3d.io.read_point_cloud(translated_kinect_scan_path)\n",
    "direction_scan, centroid_scan, peduncle_point_scan = compute_line_scan(translated_kinect_scan)\n",
    "line_points = [centroid_scan, peduncle_point_scan]  \n",
    "line_set = o3d.geometry.LineSet()\n",
    "line_set.points = o3d.utility.Vector3dVector(line_points)\n",
    "line_set.lines = o3d.utility.Vector2iVector([[0, 1]])\n",
    "o3d.visualization.draw_geometries([translated_kinect_scan, line_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aligning the models based on their center lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rotation(v1, v2):\n",
    "    # Normalize vectors\n",
    "    v1 = v1 / np.linalg.norm(v1)\n",
    "    v2 = v2 / np.linalg.norm(v2)\n",
    "    \n",
    "    # Compute rotation axis\n",
    "    rotation_axis = np.cross(v1, v2)\n",
    "    rotation_axis = rotation_axis / np.linalg.norm(rotation_axis)\n",
    "    \n",
    "    # Compute rotation angle\n",
    "    cos_angle = np.dot(v1, v2)\n",
    "    rotation_angle = np.arccos(cos_angle)\n",
    "    \n",
    "    return rotation_axis, rotation_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axis_angle_to_rotation_matrix(axis, angle):\n",
    "    # Using the Rodrigues' rotation formula\n",
    "    K = np.array([\n",
    "        [0, -axis[2], axis[1]],\n",
    "        [axis[2], 0, -axis[0]],\n",
    "        [-axis[1], axis[0], 0]\n",
    "    ])\n",
    "    I = np.eye(3)\n",
    "    \n",
    "    R = I + np.sin(angle) * K + (1 - np.cos(angle)) * np.dot(K, K)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotation(pcd, rotation_matrix):\n",
    "    return np.dot(pcd, rotation_matrix.T)  # .T is for transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_direction = direction / np.linalg.norm(direction)\n",
    "target_direction = direction_scan / np.linalg.norm(direction_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the rotation\n",
    "axis, angle = compute_rotation(initial_direction, target_direction)\n",
    "R = axis_angle_to_rotation_matrix(axis, angle)\n",
    "\n",
    "source_points = np.asarray(source.points)\n",
    "\n",
    "# Apply rotation to 3D model\n",
    "rotated_points_np = apply_rotation(source_points, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_pcd = o3d.geometry.PointCloud()\n",
    "rotated_pcd.points = o3d.utility.Vector3dVector(rotated_points_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([rotated_pcd, translated_kinect_scan])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having performed the initial alignment, we can see that the peduncles are in opposite sides of the orientations. So... We need to rotate one of the two in the Z-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix_around_y(angle_rad):\n",
    "    return np.array([\n",
    "        [np.cos(angle_rad), 0, np.sin(angle_rad)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(angle_rad), 0, np.cos(angle_rad)]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix_around_x(angle_rad):\n",
    "    return np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(angle_rad), -np.sin(angle_rad)],\n",
    "        [0, np.sin(angle_rad), np.cos(angle_rad)]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix_around_z(angle_rad):\n",
    "    return np.array([\n",
    "        [np.cos(angle_rad), -np.sin(angle_rad), 0],\n",
    "        [np.sin(angle_rad), np.cos(angle_rad), 0],\n",
    "        [0, 0, 1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_180_degrees = (np.pi)  # 180° in radians\n",
    "R_180 = rotation_matrix_around_z(angle_180_degrees)\n",
    "flipped_points_np = apply_rotation(rotated_points_np, R_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_pcd = o3d.geometry.PointCloud()\n",
    "flipped_pcd.points = o3d.utility.Vector3dVector(flipped_points_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([flipped_pcd, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_alignment_path = \"C:\\\\Users\\\\gusta\\\\Desktop\\\\ITESM_Desktop\\\\maestria\\\\tesis\\\\TercerSemestre\\\\realTimeICP\\\\refined_one_img_process\\\\results\\\\starting_alignment_model\\\\starting_alignment_model.ply\"\n",
    "o3d.io.write_point_cloud(starting_alignment_path, flipped_pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the desired orientation is achieved, now we can scan our model based on the bounding boxes of the kinect scan. \n",
    "\n",
    "This is done to make the model be as close in dimensions to the kinect scan as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the flipped model is \"flipped_pcd\"\n",
    "def get_dimensions(pcd):\n",
    "    bounding_box = pcd.get_axis_aligned_bounding_box()\n",
    "    return bounding_box.get_extent()\n",
    "\n",
    "\n",
    "def scale_point_cloud(source_pcd, target_dimensions, source_dimensions=None):\n",
    "    if source_dimensions is None:\n",
    "        source_dimensions = get_dimensions(source_pcd)\n",
    "    \n",
    "    scale_factors = [\n",
    "        target_dimensions[i] / source_dimensions[i]\n",
    "        for i in range(3)\n",
    "    ]\n",
    "\n",
    "    scaled_points = [\n",
    "        [scale_factors[j] * pt[j] for j in range(3)]\n",
    "        for pt in source_pcd.points\n",
    "    ]\n",
    "\n",
    "    scaled_pcd = o3d.geometry.PointCloud()\n",
    "    scaled_pcd.points = o3d.utility.Vector3dVector(scaled_points)\n",
    "    return scaled_pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dimensions = get_dimensions(translated_kinect_scan)\n",
    "scaled_pcd2 = scale_point_cloud(flipped_pcd, target_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 10000 points."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We paint the point clouds to be able to distinguish between them\n",
    "translated_kinect_scan.paint_uniform_color([1, 0, 0])  # Paint the first point cloud red\n",
    "scaled_pcd2.paint_uniform_color([0, 1, 0])  # Paint the scaled point cloud green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([translated_kinect_scan, scaled_pcd2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_aligned_model_path = \"C:\\\\Users\\\\gusta\\\\Desktop\\\\ITESM_Desktop\\\\maestria\\\\tesis\\\\TercerSemestre\\\\realTimeICP\\\\refined_one_img_process\\\\results\\\\scaled_aligned_model\\\\pancake_right_inclination.ply\"\n",
    "o3d.io.write_point_cloud(scaled_aligned_model_path, scaled_pcd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models have now been scaled. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "o3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
